---
title:  "Linear algebra and linear system - Linear Transformations amd Matrices"
excerpt: "학부연구생 스터디"
use_math: true
categories:
  - Linear algebra and linear system
---

### Dimensions

basis는 일정하지 않지만 개수는 항상 일정하고 주어진 벡터의 demension 이라고 할 수 있다 라는 핵심 내용을 설명합니다.


**Theorem**
<p align="center"><img src="https://user-images.githubusercontent.com/54671691/107179409-c82c3880-6a19-11eb-8811-1e3a0b711235.JPG" width = "400" ></p>

vector space X가 있고 basis V가 주어졌을 때 X 에서 꺼낸 임의의 vector x는 v들의 선형 조합으로 항상 표현할 수 있고 유일하다.

pf 1)

Q : x는 v들의 선형조합으로 표현이 가능한가?

A : yes, basis의 정의가 x를 span하는 거니까 임의의 원소를 꺼내면 언제나 선형조합으로 표현할 수 있다.

Q : $\alpha_i$ 들이 과연 유일한가?
A : 또다른 표현 $\beta$ 가 있다고 생각하면

$x=\alpha_1v_1+ \cdots \alpha_nv_n$

$x=\beta_1v_1+ \cdots \beta_nv_n$

$(\alpha_1-\beta_1)v_1 + \cdots + (\alpha_n - \beta_n)v_n=0$

v는 basis라고 독립이고 따라서 계수들은 0이되고 유일하다는 것을 알 수 있다.

pf 2)

Q : $v_i$를 span하면 X가 된다.

A : X에서 임의의 원소 x를 꺼내서 유일하게 

$x=\alpha_1v_1+ \cdots \alpha_nv_n$ 처럼 표현된다는 뜻은 

x는 $v_i$들의 선형 조합이 된다는 뜻이다.

따라서 $v_i$들을 span하면 X가 된다.

Q : $v_i$ 들이 독립인가?
A : vector space에 항상 포함된 0을 X에서 꺼내면

$0 = \alpha_1v_1+ \cdots \alpha_nv_n$ 다음과 같이 유일하게 표현된다는것이 가정이다.

이 식을 만족하는 $\alpha$ 는 0이고 이것이 유일하다면 $v_i$ 들은 독립임을 보여줄 수 있다.

<br>

Theorem의 유용성 : Basis가 주어지면, vector space내의 모든 원소는 linear combination으로 유일하게 표현할 수 있다.

Basis가 주어지면, vector space내의 원소는 column vector와 동일하게 볼 수 있다.

장점으로는 x를 가지고 사용하는것 대산에 $\alpha_1 ~ \alpha_n$을 가지고 operation을 할 수 있다는 것이다.

<br>
<br>

**Replacement Theorem**
<p align="center"><img src="https://user-images.githubusercontent.com/54671691/107180779-c617a900-6a1c-11eb-8da6-b1e62c19f378.JPG" width = "400" ></p>

pf) $m>n$ 으로 가정하고 모순을 찾는다.

$V_k := \{u_1, \cdots u_k, v_{k+1}, \cdots , v_n\}$

$0 \le k < n $

suppose : $span(V_k)=X$

$\{u_1, \cdots u_k,u_{k+1}, v_{k+1}, \cdots , v_n\}$

이때 추가된 $u_{k+1}$ 은 나머지의 선형조합으로 표현할 수 있습니다.
즉 선형 종속이다.

그러면 다음 식을 만족하는 0이 아닌 $\alpha$ 가 있다.

$\alpha_1u_1 + \cdots + \alpha_k u_k + \alpha_{k+1}u_{k+1}+\alpha_{k+2}v_{k+1} + \cdots + \alpha_{n+1}v_n =0$

그러면 0이 아닌 $\alpha$가 어느 구간에 있는지를 파악해야하는데 먼저

$\alpha_{k+2}$ 에서 $ \alpha_{n+1}$ 구간이 전부 0이라면 나머지가 0이되어야 하는데 $\alpha_1$ 부터 $\alpha_{k+1}$ 는 모든 u가 서로 독립임으로 모순이다.

따라서 $\alpha_{k+2}$ ~ $ \alpha_{n+1}$ 구간에서 0이 아닌 $\alpha$ 가 있다.

만약 $\alpha_{k+2}$ 가 0이 아니라면 $v_{k+1}$이 선형조합으로 표현가능하다. 그 값을 뺀것을 다시 정의하면 

$V_{k+1} := \{u_1, \cdots u_{k+1}, v_{k+2}, \cdots , v_n\}$

또한 다음을 만족한다 
$span(v_{k+1})=X$ 

k가 n이 될때까지 반복하면

$span(v_n)=X$ 

이때 처음 가정에서 $m > n$ 이라고 했기 때문에

$u_{n+1} \in X = span(v_n)$ 이 되는데 이는 모순이다.

$v_n$ 은 n개의 u로 이루어져 있는데 X 에 $u_{n+1}$이 속한다는 의미는 선형 조합으로 $u_{n+1}$을 표현할 수 있다는 의미고 선형 종속이다.

처음에 u 벡터들은 전부 선형 독립이라고 했으므로 이는 모순이 된다.
따라서 $m > n$일 수 없다.

<br>
<br>


**Theorem**

<p align="center"><img src="https://user-images.githubusercontent.com/54671691/107197473-4945f880-6a37-11eb-822d-455a420e4e21.JPG" width = "400" ></p>

위에서 사용했던 replacement theorem을 두번 사용하면 다음을 알 수 있다. 즉 basis는 다를 수 있지만 그 개수는 항상 똑같다.

<br>
<br>

**Dimension**

<p align="center"><img src="https://user-images.githubusercontent.com/54671691/107197687-8f9b5780-6a37-11eb-8401-9e79d662bb8a.JPG" width = "400" ></p>

위의 불변의 물리량, basis의 개수를 dimension이라고 한다.

dim(X) : X라는 vector space의 basis의 개수

<br>
<br>

ex)

$R^n$ : n차 vector space

$R^{mxn}$ : mxn차 vector space

$R_3[ s]$ : 3차 vector space

$R[s ]$ : 무한차수 vector space


---

### Change of Basis

vector space X 가 주어졌을 때 basis V가 있고 V를 이용해서 X의 원소를 유일하게 표현할 수 있는 $\alpha$ 가 있고 다른 basis W가 있고 $\alpha$ 와 마찬가지로 $\beta$가 있다고 생각해보면 다음을 볼 수 있습니다.

$x\in X$

$x = [v_1 \cdots v_n] \alpha = [w_1 \cdots w_n] \beta$

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107200642-4d741500-6a3b-11eb-92bf-d81715588a54.JPG" width = "200" ></p>

즉 v를 w의 선형조합으로 표현했다. P는 scalar

$i = 1 \cdots n, P_{1i}$ 는 유일하게 결정

표현을 다음과 같이 할 수도 있다.

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107201265-1b16e780-6a3c-11eb-8ec1-b26d0fff531c.JPG" width = "350" ></p>

가장 우측 행렬을 P행렬이라고 하고 $v_1 \cdots v_n$ 을 위의 식에 대입하면

$x = [w_1 \cdots w_n]P\alpha = [w_1 \cdots w_n]\beta$ 가 된다

표현이 유일하다고 했기 때문에 $P\alpha = \beta$ 가 성립한다.

또한 $w_1 \cdots w_n$을 대치하면 $\alpha = Q\beta$ 와 같이 생각할 수 있고 또한 $PQ=I$가 됨을 보일 수도 있다.


ex)

$R_3[s],R$

$v=\{1,s,s^2\}$

$w=\{1+2,s+s^2,s^2\}$

$R_3[s]\ni x = \ 2s^2+2s+5$

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107203477-d80a4380-6a3e-11eb-8f8d-cd7912d7ceef.JPG" width = "180" ></p>

그렇다면 $\beta$를 구하기 위해서 Q를 구하면

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107204194-b8274f80-6a3f-11eb-9c96-4edc4319c1a8.JPG" width = "180" ></p>

따라서 $\beta=Q^{-1}\alpha$ 와 같이 구할 수 있다.

---
### Linear Transformations, null Spaces, and Range Spaces

<br>

**Linear Transformation**

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107204483-0e948e00-6a40-11eb-84ac-f26e441192f2.JPG" width = "400" ></p>


vector space X 원소를 vector space Y의 원소로 넘겨주는것이 mapping이고 위에 나온 조건 두개를 만족하는 mapping 을 선형변환이라고 합니다.

선형변환이면 만족하는 성질
$L(0)=L(0x)=0L(x)=0 $


<br>
<br>

**Range space, Null space**

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107206845-ed816c80-6a42-11eb-9471-3b60398bf75d.JPG" width = "400" ></p>

$N(L)=\{x: Lx=0\}$

$R(L)=\{y: y=Lx, x\in X\}$

<br>
<br>

**Theorem**

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107207301-7e584800-6a43-11eb-921b-cc1deb4e8fe7.JPG" width = "400" ></p>

1번이면 2번이고 2번이면 3번이다.

1 : 도메인의 원소가 다르면 타겟의 원소도 다르다
2 : Null Space가 0 밖에 없다 => trivial
3 : x라는 vector space에서 임의의 n개의 선형독립인 벡터를 꺼내서 L mapping을 통해서 y도메인으로 넘기고 나면 그 y 도메인에 넘어간 vector들이 선형독립이다.

pf 2->1)

$x,v \in X$

$Lx = Lv \Rightarrow x=v$ 을 보이면 된다.

이항하면 다음과 같고 Null Space는 0밖에 없으므로 one to one이라는 것을 알 수 있다.

$L(x-v)=0$ 

<br>

pf 1->2)
Null space가 tirivial이 아니라면

$N(L)\ne \{0\}, N(L)\ni x\ne0$ 다음과 같은 x가 있을 수 있다.

다음 두식을 둘다 만족하므로 2번이 아니면 1번이 아니다, 즉 1번이면 2번이다.

$Lx =0$

$L0 =0$

<br>

pf 2-> 3)

$v_i \in X$ (독립)

$w_i = Lv_i,~ i=1\cdots,k$

$\alpha_1w_1 + \cdots + \alpha_kw_k=0$ 이때 위의 식을 사용해서 w를 바꾸면

$L(\alpha_1v_1 + \cdots + \alpha_kv_k)=0$

Null space는 0밖에 없으므로 $(\alpha_1v_1 + \cdots + \alpha_kv_k)=0$

또한 v가 독립이므로 모든 $\alpha$는 0이고 따라서 w도 독립이다.

<br>

pf 3 -> 2)

2번을 부정함으로써 보여준다.

$N(L) \ne {0} , ~~ N(L) \ni x\ne 0$ 가 존재

$N(L) \subset X$ 이므로 $x \in X$

$\{x\} : indep$

$\{Lx\}=\{0\} : dep$

선형 독립인 벡터들을 mapping하면 선형독립인 벡터들이 되야하는데 선형 종속인 벡터들이 됐다.


<br>
<br>

**Nullity and rank**

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107216966-da759900-6a50-11eb-9956-f37c2279ac79.JPG" width = "400" ></p>

N(L) 과 R(L)은 Subspace여서 Vectorspace이다.

Vectorspace이면 dim을 생각할 수 있다.

dim(N(L)) = nullity of L
dim(R(L)) = rank of L


<br>

**Dimension Theorem**

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107218601-2b868c80-6a53-11eb-8f05-12efabaaea21.JPG" width = "400" ></p>

pf)

$\{v_1 \cdots v_n\}$ basis of X (dimension = n)

$\{u_1 \cdots u_m\}$ basis of N(L) $\subset X$

N(L)이 X의 subset이므로 $m\le n$  

replacement theorem을 사용해서 X의 basis를 바꾸면

$\{u_1, \cdots, u_m, v_{m+1}, \cdots , v_n\}$ : basis of X

이때 S를 다음과 같이 정의한다

$S \triangleq \{Lv_{m+1}, \cdots , Lv_n\} \subset Y $

1) span(S) = R(L)
2) S : indep

위의 두 가정이 참이라면 R(L)의 basis가 S라는 것을 의미합니다.
또한 rank(L)의 dim은 n-m이 됩니다.

따라서 rank(L)이 n-m이 되는 것을 보이게 됩니다.


(1) $span(S) \ni x$

$x=\alpha_1Lv_{m+1}+ \cdots + \alpha_{n-m}Lv_n$

$=L(\alpha_1v_{m+1}+\cdots + \alpha_{n-m}v_n)$

따라서 $x\in R(L)$ 

반대로 R(L) 에서 원소를 꺼내서 확인하면 span(S)에 속한다는 것을 알 수 있고 따라서 성립한다.

<br>

(2)

$\alpha_1LV_{m+1}+ \cdots + \alpha_{n-m}LV_n = 0$

$\Rightarrow L(\alpha_1V_{m+1}+\cdots + \alpha_{n-m}V_n)=0$

따라서 $\alpha_1V_{m+1}+ \cdots + \alpha_{n-m}V_n$ 은 L의 Null space의 원소입니다.

따라서 $\alpha_1V_{m+1}+ \cdots + \alpha_{n-m}V_n$ 은

$\beta_1u_1 + \cdots + \beta_m u_m$이 될 수 밖에 없습니다.

앞에서 $u_1$ 부터 $v_n$ 까지는 X의 basis라고 했으므로 전부 독립입니다

---

### Matrix Representations of Linear Transformations

vector space X를 vector space Y로 선형변환해주는 L이 있다고 할때 
X에는 basis V 그리고 선형조합의 계수 $\alpha$, Y에는 basis W 그리고 선형조합의 계수 $\beta$ 가 있을 때 $\alpha, \beta$의 관계를 찾으면 선형변환이 어떤 역할을 해주는지 알 수 있습니다.

$\alpha, \beta$의 관계는 $\beta = A\alpha$ 로 표현할 수 있으며 A는 어떠한 행렬입니다.

basis에 따라 값이 달라지므로 $[L]_{v,w}$와 같이 표현하기도 합니다.

$[v_1 \cdots v_n]\alpha = x \in X$

$[w_1 \cdots w_n]\beta = y=Lx \in Y$

선형 변환을 사용하면 다음과 같습니다.

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107225744-87a1de80-6a5c-11eb-9e1e-817bcd7d89dd.JPG" width = "500" ></p>

이때 가장 우측 행렬이 A 행렬이 됩니다.

표현식이 되는 이유는
<p align=""><img src="https://user-images.githubusercontent.com/54671691/107226680-c5533700-6a5d-11eb-86d7-14a71ab3aba9.JPG" width = "600" ></p>

이때 $L(x)$ 는 원래 $y=[w_1 \cdots w_n] \beta $ 이므로 똑같은 object에 대해 두가지 표현식이 존재하는 건데 basis가 같으면 그 계수가 같으므로 따라서

$\beta = A\alpha$ 된다.

ex) 미분하는 operation을 행렬 곱으로 표현해 봅시다.

$R_3[s]$ 를 $R_2[s]$ 로 변환하는, 각각의 basis는 $\{1,s,s^2\}, \{1,s\}$ 입니다.

미분 operation은 선형 변환입니다.

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107227679-0b5cca80-6a5f-11eb-9015-f2d6797c1138.JPG" width = "300" ></p>

$2S^2 + 5S+3 \in R_3[ S]$

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107235426-03edef00-6a68-11eb-8883-c069d600eb93.JPG" width = "180" ></p>

나타내는 값은 $4S+5$ 가되어 미분값이랑 같은 것을 알 수 있습니다.


<br>
<br>

**Similarity transformation**

<p align=""><img src="https://user-images.githubusercontent.com/54671691/107236463-1583c680-6a69-11eb-931c-a5f8fd2c70ce.JPG" width = "400" ></p>


<p align=""><img src="https://user-images.githubusercontent.com/54671691/107236086-b1f99900-6a68-11eb-8df3-fc955796244f.jpg" width = "180" ></p>

다음과 같은 관계에서 

$\beta = P^{-1}BP\alpha$ 이므로

따라서 $A = P^{-1}BP$ : 유사행렬

Matrix representation과 Similarity transformation의 관계
: Basis가 달라지면, Linear trasformation의 Matrix representation이 Similarity transformation의 형태로 나타난다